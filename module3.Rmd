---
title: "module3"
author: "GroupeB"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r importation-dataset}
df <- read.csv("College.csv")
head(df)
```

## Exercice 1 :

Pour chacune des parties (a) à (d), indiquez si nous nous attendons généralement à ce que les performances d’une méthode d’apprentissage statistique flexible soient meilleures ou pires que celles d’une méthode rigide. Justifiez votre réponse.

(a)- La taille de l’échantillon 
est extrêmement grande et le nombre de prédicteurs 
est petit.

**Rép** : Si on a beaucoup de données et peu de prédicteurs, c'est mieux d'opter pour une méthode rigide. Cela évitera 
de trop s'adapter aux donner et créer des problèmes de sur ajustement.

(b)- Le nombre de prédicteurs 
est extrêmement grand et le nombre d’observations 
est petit

**Rép** : La méthode rigide est encore mieux. En effet, avec peu d'observations, une méthode flexible a 
assez de lattitude pour s'ajuster à l'ensemble des observations et mal généraliser. 

(c)- La relation entre les prédicteurs et la réponse est fortement non linéaire.

**Rép**: La méthode flexible est meilleure afin de mieux capturer la complexité des relations.

(d)-La variance des termes d’erreur, c’est-à-dire 
, est extrêmement élevé

**Rép** : Si la variance est élevé cela signifie que il y a beaucoup de bruits donc en utilisant la méthode flexible, elle risque de faire un surajustement. Alors la méthode rigide est mieux adapter en sachant qu'elle est moins sensible au bruit.


## Exercice 2 :

Expliquez si chaque scénario est un problème de classification ou de régression, et indiquez si nous sommes plus intéressés par l’inférence ou la prédiction. 


(a)-Nous collectons un ensemble de données sur les 500 plus grandes entreprises aux États-Unis. Pour chaque entreprise, nous enregistrons les bénéfices, le nombre d’employés, le secteur d’activité et le salaire du PDG. Nous souhaitons comprendre quels facteurs affectent le salaire du PDG.

**Rep**: C'est un problème de **regression**, par ce que le salaire est quantitatif continue.
On veut faire une **inférence**. par ce que ce n'est pas le salaire en soi qui nous interesse, mais les variables qui le
déterminent. La taille de l'échantillon n est 500 et le nombre de prédicteurs p est 4.

(b)-Nous envisageons de lancer un nouveau produit et souhaitons savoir si ce sera un succès ou un échec. Nous collectons des données sur 20 produits similaires précédemment lancés. Pour chaque produit, nous avons enregistré s’il s’agissait d’un succès ou d’un échec, le prix facturé pour le produit, le budget marketing, le prix de la concurrence et dix autres variables. 

**Rép**: C'est  un problème de **classification** (Succès ou échec), on veut faire une **prédiction**. La taille de l'échantillon n est 20 et le nombre de prédicteurs p est 13.

(c)- Nous souhaitons prédire la variation en % du taux de change USD/euro par rapport aux variations hebdomadaires des marchés boursiers mondiaux. C’est pourquoi nous collectons des données hebdomadaires pour toute l’année 2012. Pour chaque semaine, nous enregistrons le % de variation de la parité USD/Euro, le % de variation du marché américain, le % de variation du marché britannique et le % de variation du marché allemand.

**Rép**: problème de **regression**, on veut faire une **prédiction**.La taille de l'échantillon n est ... et le nombre de prédicteurs p est 4.

## Exercice 3

```{r}
df <- data.frame(
  X1 = c(0, 2, 0, 0, -1, 1),
  X2 = c(3, 0, 1, 1, 0, 1),
  X3 = c(0, 0, 3, 2, 1, 1),
  Y = c("Rouge", "Rouge", "Rouge", "Vert", "Vert", "Rouge")
)
print(df)
```

####a-Calculons la distance euclidienne entre chaque observation et le point de test

on sait que X1=X2=X3=0 donc le point de test (0,0,0). 

On sait que la distance euclidienne entre deux observations est: distance <- sqrt(sum((x - y)^2))


```{r}
test <- c(0, 0, 0)
distances <- sqrt(rowSums((df[, 1:3] - test)^2))
df$Distance <- distances
print(df)
```
b-Quelle est notre prédiction avec K=1 Pourquoi?

####Pour K=1  la prédiction est  vert

Car le point le plus proche de 1 est la distance de l'observation 5 de coordonner (−1,0,1)  qui de 1.41  et de couleur vert.

c-Quelle est notre prédiction avec K=3 Pourquoi?

####Pour K=3  La prediction est  Rouge 

Car les 3 points les plus proches sont les observations 5, 6 et 2 avec des distances respectives
Observation 5 (−1,0,1) distance 1.41 de couleur  Vert
Observation 6 (1,1,1) distance 1.73 de couleur Rouge
Observation 2 (2,0,0) distance 2 de couleur Rouge 
et la couleur dominante est Rouge

d-Si la règle de décision de Bayes dans ce problème est hautement non linéaire, alors nous attendrions-nous à ce que la meilleure valeur pour soit grande ou petite ? Pourquoi?

Si la règle de décision de Bayes est hautement non linéaire, une petite valeur de k est préférable car Cela permet de mieux capturer les variations locales et les plus  frontières complexes.

## Exercice 4 

LA question est manquante

## Exercice 5 
a-Aperçu des courbes typiques 

```{r}
# Chargement des bibliothèques
library(ggplot2)
library(dplyr)

# Définition la flexibilité (axe X)
variabitite <- seq(0, 15, length.out = 500)

# Définition des courbes
erreur_bayes <- rep(0.5, length(variabitite))  # Erreur de Bayes (constante)
biais_carre <- exp(-0.2 * variabitite)       # Biais(carré) diminue avec la flexibilité
variance <- 0.1 * exp(0.2 * variabitite)      # Variance augmente avec la flexibilité
erreur_entrainement <- 0.2 * exp(-0.5 * variabitite) # Erreur d'entraînement diminue
erreur_test <- biais_carre + variance + erreur_bayes  # Erreur de test = biais_carre + variance + erreur irréductible

# Création du DataFrame
df <- data.frame(
  variabitite = rep(variabitite, 5),
  erreur = c(biais_carre, variance, erreur_entrainement, erreur_test, erreur_bayes),
  type = rep(c("Biais carre", "Variance", "Erreur d'entraînement", "Erreur de test", "Erreur de Bayes"), each = length(variabitite))
)

# Tracé avec ggplot
ggplot(df, aes(x = variabitite, y = erreur, color = type, linetype = type)) +
  geom_line(size = 1) +
  labs(title = "Compromis Biais-Variance et Erreurs en fonction de la Flexibilité",
       x = "Flexibilité du modèle",
       y = "Erreur") +
  theme_minimal() #+
  #theme(legend.title = element_blank())

```
b- Explication de la forme de chacune des cinq courbes affichées précédement:   
- Erreur de Bayes est constante, c'est l'erreur irréductible, ce qui se représente une droite horizontale
- Biais carré diminue progressivement avec la flexibilité qui augmente ce qui est représenté par une fonction exponentielle
- Variance augmente progressivement avec la flexibilité qui augmente
- Erreur d'entraînement diminue avec la flexibilité qui augmente
- Erreur de test est égale à biais_carre + variance + erreur irréductible, elle diminue à mesure que la flexibilité augmente atteignant sont minimum avant de croitre de nouveau à mesure que la flexibilité augmente.

## Exercice 6
Description de trois applications réelles dans lesquelles la classification pourrait êre utile
1- Classification des courriels (Spam vs Non-Spam)
Problème : Filtrer les courriels indésirables (spam).
Réponse (variable cible) :
1 = Spam
0 = Non-Spam
Prédicteurs :
Fréquence de certains mots-clés
Adresse e-mail de l’expéditeur
Présence de liens ou pièces jointes suspectes
Longueur du message
But : Prédiction
L’objectif est de classer automatiquement les e-mails en fonction de leur contenu et d'éviter que des spams n’atteignent la boîte de réception.
Il s’agit uniquement d’une prédiction, sans besoin d’inférer les causes sous-jacentes du spam.

2-Détection des faux passeport (Contrefait vs Original)
Problème: Empêcher l'embarquement des passagers ayant de faux passeports
Réponse(variable cible):
1 = Contrefait
0 = Original
Prédicteurs:
Numéro du passeport  
Présence des différents hologrammes de sécurité
Nombres de visa
But: Prédiction
L'objectif est de reconnaitre automatique les faux passeports et éviter ainsi que leur détenteurs ne puissent voyager en utilisant la compagnie. Il s'agit ici uniquement de prédiction, sans inférence sur les causes sous-jacentes.

3-La détection de soudure défectueuses
Problème: Empêcher la commercialisation d'équipement indusstriel non conforme
Réponse(variable cible):
1 = Défectueuse
0 = Conforme
Prédicteurs:
taille de la soudure
image de la soudure
temps de refroidissement
données ultrasons
But: Prédiction et inférence
L'objectif est de détecter précossement les défauts de soudure sur les pièces critiques avant d'éviter des bris sur des équipements stratégique de production. On s'interesse aussi aux causes.

Description de trois applications réelles dans lesquelles la regression pourrait êre utile
1- Prévision des prix des maisons
Problème : Estimer le prix d’une maison en fonction de ses caractéristiques.
Réponse (variable cible) :
Prix de vente (en dollars)
Prédicteurs :
Surface habitable (m²)
Nombre de chambres
Nombre de salles de bains
Localisation (quartier, proximité aux commerces, écoles)
Année de construction
État du bien (neuf, rénové, ancien)
But : Prédiction
L’objectif est de prédire le prix d’une maison en fonction des caractéristiques observées.
Il n'est pas important de comprendre l’impact exact de chaque variable. Nous voulons juste obtenir une estimation précise du prix de la maison.


2- Prévision des prix d'une action à la bourse
Problème: Estimer le prix d'une action en fonction des ses caractéristiques.
Réponse(variable cible):
Prix de vente de l'action
Prédicteurs:
directeur general
nombre d'employés  
nouveaux contrat
But: Prédiction
L'objectif est de prédire le prix de vente d'une action.

3- Prévision du revenu 
Problème: Estimer le revenu futur d'un individu en fonction de ses caractéristiques.
Réponse(variable cible):
Montant du revenu annuel
Prédicteurs:
Niveau de scolaire
date dernière formation
domaine d'activité
ancienneté
revenu actuel
employeur(salarié, travailleur autonome)
But: Prédiction
L'objectif est de déterminer le salaire future d'une personne connaissant ses caractéristiques. 
On ne s'interesse par aux niveau d'impacte de chacune de ses différentes variable.

## Exercice 7 
Avantages et inconvénients d’une approche très flexible (par rapport à une approche moins flexible) de régression ou de classification
Avantages:
Faible biais
Meilleur ajustement aux données (bon apprentissage)
Performance accrue sur des données riches
Inconvenients:
Overfitting(sur ajustement au données d'entrainement)
consommation accrue des ressources de calculs
Besoin d'une grande quantité de données

On choisira un modèle plus flexible lorsque l'on a beaucoup de données et que l'on a pas une relation évidente
entre les variable.
On choisira un modèle moins fexible lorsque l'on a peut de données et que l'interprétation des données est cruciale.



## Exercice 8
Avantages d’une approche paramétrique en régression ou classification:
- Moins de risque de sur-apprentissage (Overfitting)
- Rapidité d’entraînement et d’inférence
- Facilité d’interprétation 
- Moins de données nécessaires
Inconvenients d’une approche paramétrique en régression ou classification:
- Plus de risque de sous-apprentissage (Underfitting)
- s'ajuste mal sur des données complexes

## Pratique

```{r partie a et b}
#importation des données
college <- read.csv("College.csv")
#visualisation des données
#View(college)
#changement du nom des lignes
rownames(college) = college[,1]
# suppression de la colonne x
college= college[, -1]
View(college)

```

c
```{r partie c}
#résumé numérique des données
table(summary(college))

#Afficher la matrice de nuage de points pour les 10 premières colonnes numériques
college_10_first = college[,1:10]
college_10_first_numeric <- college_10_first[, sapply(college_10_first, is.numeric)]  
pairs(college_10_first_numeric, main="matrice de nuage de points pour les 10 premières colonnes numériques")

#boxplot cote à cote
#plot(Outstate~Private, data=college, main= "Boxplot de Outstate par Private",
#     xlab = "Private(Yes/No)", ylab = "Outstate", col=c("lightblue", "lightgreen"),
#     ylim(range(college$Outstate,na.rm = TRUE)))
college$Private = as.factor(college$Private)
plot(Outstate~Private, data = college, main = "Boxplot de Outstate par Private",
    xlab = "Private (Yes/No)", ylab = "Outstate", col = c("lightblue", "lightgreen"),
    ylim(range(college$Outstate,na.rm = TRUE)))
#ggplot(college, aes(x = Private, y = Outstate, fill = Private)) +
#  geom_boxplot() +
#  theme_minimal() +
#  labs(title = "Boxplots de Outstate en fonction de Private") +
#  facet_wrap(~Private, scales = "fixed")


#creation de la variable Elite
Elite <- rep("No", nrow(college))
Elite[college$Top10perc > 50] <- "Yes"
Elite <- as.factor(Elite)
college <- data.frame(college, Elite)

#Nombre d'université d'elite
summary(college$Elite)[2]

#boxplot cote à cote de Outstate et Elite
plot(Outstate~Elite, data = college, main = "Boxplot de Outstate par Elite",
    xlab = "Elite (Yes/No)", ylab = "Outstate", col = c("lightblue", "lightgreen"),
    ylim(range(college$Outstate,na.rm = TRUE)))

#ggplot(college, aes(x = Elite, y = Outstate, fill = Elite)) +
#  geom_boxplot() +
#  theme_minimal() +
#  labs(title = "Boxplots de Outstate en fonction de ") +
#  facet_wrap(~Private, scales = "fixed")


```


```{r exploration}

```
